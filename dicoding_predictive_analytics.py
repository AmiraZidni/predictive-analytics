# -*- coding: utf-8 -*-
"""Dicoding: Predictive Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YCOXRky4iMvcS3kl-NyJIn6KPOyBY-bK

Nama: Zidni Iman Sholihati

Dicoding profil: [URL](https://www.dicoding.com/users/amirah_zidni)

Email: zidni.imani@gmail.com / m1821860@bangkit.academy

Kriteria submission:

1. Project merupakan hasil pekerjaan sendiri. 
2. Project belum pernah digunakan untuk submission kelas Machine Learning di Dicoding dan belum pernah dipublikasikan di platform manapun.
3. Dataset yang dipakai merupakan data kuantitatif (minimum 500 sampel data).
4. Memberikan dokumentasi setiap cell code dengan menggunakan text cell pada notebook (.ipynb) agar proyek mudah dibaca.
5. Menentukan solusi permasalahan menggunakan pendekatan machine learning atau deep learning dengan memilih salah satu dari penyelesaian berikut:
  *   Klasifikasi
  *   Regresi
  *   Clustering
  *   Time series dan forecasting
6. Membuat draf laporan proyek machine learning yang menjelaskan alur proyek Anda dari mulai pemilihan domain permasalahan (problem domain), data understanding, data preparation, modeling, hingga tahap evaluasi.

Saran submission:
* Menerapkan Rubrik/Kriteria Penilaian (Tambahan) untuk mendapatkan skala penilaian (bintang) yang lebih tinggi.
* Semua ketentuan terpenuhi, menerapkan minimal lima (5) Rubrik Penilaian (Tambahan) pada laporan.

# Domain Proyek
Domain proyek ini akan membahas bidang ekonomi dan bisnis dengan judul **"Prediksi Keputusan Pelanggan Dalam Pembelian Asuransi Perjalanan"**.

Latar belakang proyek ini adalah diperlukannya pemetaan pelanggan yang memiliki kemungkinan untuk membeli asuransi perjalanan. Asuransi perjalanan memungkinkan orang bepergian mendapatkan perlindungan selama melakukan perjalanan dari kejadian tidak terduga seperti sakit, keterlambatan pesawat, atau hal tidak terduga yang mungkin terjadi dengan rumah yang ditinggal [[1]](https://kc.umn.ac.id/13580/).

Hasil proyek ini adalah sebuah *machine learning* yang dapat digunakan sebagai pendukung pembuatan keputusan sebuah perusahaan asuransi perjalanan dalam menyasar pelanggannya mengingat kemungkinan bidang bisnis ini akan diprediksi kembali naik setelah hampir punah selama pandemi [[2]](https://www.tandfonline.com/doi/full/10.1080/02513625.2020.1794120). Seiring pemulihan penerbangan, jasa asuransi perjalanan ini dapat menjadi produk menarik tersendiri bagi orang bepergian mengingat risiko pandemi yang membutuhkan waktu untuk kembali normal.

# Business Understanding

## Problem Statements
Dari latar belakang di atas, dapat ditarik rumusan masalah sebagai berikut:
1. Bagaimana melakukan pra-pemrosesan data asuransi perjalanan agar menghasilkan data latih bagi *machine learning* prediksi keputusan pelanggan dalam pembelian asuransi perjalanan?
2. Bagaimana membuat model *machine learning* yang mampu memprediksi keputusan pelanggan dalam pembelian asuransi perjalanan?

## Goals
Tujuan proyek yang ingin dicapai adalah:
1. Melakukan pra-pemrosesan data asuransi perjalanan agar menghasilkan data latih yang cukup bagi *machine learning* prediksi keputusan pelanggan dalam pembelian asuransi perjalanan.
2. Membuat model *machine learning* yang mampu memprediksi keputusan pelanggan dalam pembelian asuransi perjalanan dengan akurasi >= 80%.

## Solution statements
1. Pra-pemrosesan dilakukan dengan langkah-langkah berikut:
  - **Resample dataset** dengan menyeimbangkan jumlah data.
  - **Pembagian dataset** dengan data latih 80% dan data uji 20%.
  - **Standarisasi data** dengan mengubah skala data menjadi relatif sama atau mendekati distribusi normal. 
2. Pembuatan model pada proyek ini menggunakan dua model sebagai berikut:
  - **KNN**. KNN adalah algoritma yang menggunakan kesamaan fitur untuk memprediksi nilai baru. Nilai baru ini didasarkan pada seberapa mirip dengan tetangganya sejumlah  k, oleh karena itu disebut K-Nearest Neighbor.
  - **Gradient Boosting Algorithm**. Algoritma ini bekerja dengan meningkatkan (*boosting*) model yang dianggap memiliki performa rendah atau akurasi yang belum memuaskan.

# Data Understanding
Dataset proyek ini berasal dari platform Kaggle yang dipublikasi oleh TejasTheBard dengan judul [Travel Insurance Prediction Data](https://www.kaggle.com/tejashvi14/travel-insurance-prediction-data). Berdasarkan metadata, dataset ini bersumber dari basis data perusahaan perjalanan di India. Dari dataset TravelInsurancePrediction.csv yang diunduh, dataset memiliki 10 kolom dan 1987 baris dengan keterangan berikut:

| Fitur               | Deskripsi                                                                                             |
| --------------------| ----------------------------------------------------------------------------------------------------- |
| Index               | Indeks (urutan) data.                                                                                 |
| Age                 | Umur pelanggan.                                                                                       |
| Employment Type     | Sektor pelanggan bekerja (Pemerintah (Government Sector) atau Swasta (Private Sector/Self Employed'). |
| GraduateOrNot       | Status lulusan perguruan tinggi (Yes/No).                                                             |
| AnnualIncome        | Pendapatan tahunan (Rupee).                                                                           |
| FamilyMembers       | Jumlah anggota keluarga.                                                                              |
| ChronicDiseases     | Status ada tidaknya penyakit kronis pelanggan (asma, diabetes, darah tinggi, dll) (Yes/No).           |
| FrequentFlyer       | Status jika sering bepergian berdasarkan riwayat 2 tahun terakhir (Yes/No).                           |
| EverTravelledAbroad | Status bepergian ke luar negeri (Yes/No).                                                             |
| TravelInsurance     | Status pelanggan membeli paket asuransi (0 = Tidak, 1 = Iya).                                         |
"""

# Library
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import zipfile
import pandas as pd
import altair as alt
import seaborn as sns
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.utils import resample 
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV

from zipfile import ZipFile

"""## 1) Mengambil Dataset
Dataset yang dipakai merupakan data kuantitatif (minimum 500 sampel data).
"""

# Mengambil data kaggle
!chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && mv kaggle.json ~/.kaggle/
!kaggle datasets download tejashvi14/travel-insurance-prediction-data

# Unzip file
zip_file = '/content/travel-insurance-prediction-data.zip'
with zipfile.ZipFile(zip_file, 'r') as zip:
  zip.extractall('/content')

# Membaca csv
url = '/content/TravelInsurancePrediction.csv'
df = pd.read_csv(url)
df.sample(5)

"""## 2) Exploratory Data Analysis"""

# Informasi data
df.info()

# Memeriksa kolom numerik
df.describe()

# Memeriksa data kosong

df.isna().sum()

# Visualisasi korelasi

plt.figure(figsize=(10,10))
_ = sns.heatmap(df.corr(), cmap='coolwarm', annot=True)

"""Dari eksplorasi awal ini dan berdasarkan informasi [dataset](https://www.kaggle.com/tejashvi14/travel-insurance-prediction-data), dapat disimpulkan sebagai berikut:
1. Terdapat 1987 baris dan 10 kolom dalam dataset dengan kolom **TravelInsurance** adalah kolom target.
2. Kolom **Unnamed: 0** dapat dihapus karena hanya berisi urutan baris.
3. Tidak ada nilai kosong dalam dataset ini.
4. Kolom **Employment Type** berisi dua kategori yaitu Private Sector/Self Employed atau Government Sector.
5. Kolom **Employment Type**, **GraduateOrNot**, **FrequentFlyer**, dan **EverTravelledAbroad** dapat diganti menjadi biner (0 atau 1).
6. Nilai 1 pada TravelInsurance menandakan bahwa customer membeli asuransi travel.

"""

# Menghapus kolom yang tidak dibutuhkan dan letakkan pada dataset yang telah dibersihkan
df_clean = df.drop(["Unnamed: 0"], axis=1)

# Mengolah kolom categorical
df_clean['GovernmentSector'] = df['Employment Type'].map({'Private Sector/Self Employed':0, 'Government Sector':1})
df_clean['GraduateOrNot'] = df['GraduateOrNot'].map({'Yes':1, 'No':0})
df_clean['FrequentFlyer'] = df['FrequentFlyer'].map({'Yes':1, 'No':0})
df_clean['EverTravelledAbroad'] = df['EverTravelledAbroad'].map({'Yes':1, 'No':0})

# Menghapus kolom Employment Type yang sudah diganti GovernmentSector
df_clean.drop(["Employment Type"], axis=1, inplace=True)

df_clean.sample(5)

# Analisis univariate
df_clean.describe().transpose()

# Visualisasi korelasi kembali

plt.figure(figsize=(10,10))
_ = sns.heatmap(df_clean.corr(), cmap='coolwarm', annot=True)

# Dari hasil korelasi di atas, kita periksa satu persatu korelasinya dengan target
# Visualisasi jumlah TravelInsurance
sns.countplot(x='TravelInsurance', data=df_clean)

# Visualisasi distribusi umur
sns.countplot(x='Age', hue='TravelInsurance', data=df_clean)

# Visualisasi pembeli asuransi berdasarkan kategori GraduateOrNot
sns.countplot(x='GraduateOrNot', hue='TravelInsurance', data=df_clean)

# Visualisasi antara AnnualIncome dengan TravelInsurance
alt.Chart(df_clean, width=400).mark_boxplot().encode(
    x='TravelInsurance',
    y='AnnualIncome'
)

# Visualisasi pembeli asuransi berdasarkan FamilyMembers
sns.countplot(x='FamilyMembers', hue='TravelInsurance', data=df_clean)

# Visualisasi pembeli asuransi berdasarkan ChronicDiseases
sns.countplot(x='ChronicDiseases', hue='TravelInsurance', data=df_clean)

# Visualisasi pembeli asuransi berdasarkan kategori FrequentFlyer
sns.countplot(x='FrequentFlyer', hue='TravelInsurance', data=df_clean)

# Visualisasi pembeli asuransi berdasarkan kategori EverTravelledAbroad
sns.countplot(x='EverTravelledAbroad', hue='TravelInsurance', data=df_clean)

# Visualisasi pembeli asuransi berdasarkan kategori GovernmentSector
sns.countplot(x='GovernmentSector', hue='TravelInsurance', data=df_clean)

"""Hasil eksplorasi EDA ini menghasilkan sebagai berikut:
1. Pada kolom umur (Age), terdapat kecerendungan membeli asuransi untuk pelanggan yang berusia 33 tahun ke atas.
2. Tidak ada perbedaan signifikan di kolom GraduateOrNot, FamilyMembers, dan ChronicDiseases dalam pembelian asuransi perjalanan.
3. Terdapat kecerendungan bagi pelanggan yang sering bepergian dan melakukan penerbangan antar negara (FrequentFlyer dan EverTravelledAbroad) untuk membeli asuransi perjalanan.
4. Terdapat kecerendungan membeli asuransi untuk pelanggan yang memiliki pendapatan di atas 1.200.000.

# Data Preparation
Tahap persiapan data atau pra-pemrosesan data dilakukan dengan langkah-langkah berikut:
- **Resample dataset** dengan menyeimbangkan jumlah data. Resample dataset diperlukan untuk menghindari hasil prediksi yang bias dikarenakan kuantitas yang tidak seimbang dalam sebuah data.
- **Pembagian dataset** dengan data latih 80% dan data uji 20%. Pembagian dataset tentunya diperlukan agar model yang telah dilatih dapat diujikan seberapa akurat hasil prediksinya terhadap data baru. Dalam dataset ini rasio 80:20 dapat dikatakan masih ideal karena jumlahnya masih ribuan saja (1987 baris).
- **Standarisasi data** dengan mengubah skala data menjadi relatif sama atau mendekati distribusi normal. Tahap standarisasi digunakan untuk menyeragamkan fitur numerik dalam skala data yang sama dan dapat lebih mudah diolah saat pelatihan model.

## 3) Resample Dataset
"""

# Cek kolom
df_clean.columns

# Kolom target
target_column = 'TravelInsurance'

# Kolom fitur
feature_columns = ['Age', 'GraduateOrNot', 'AnnualIncome', 'FamilyMembers',
       'ChronicDiseases', 'FrequentFlyer', 'EverTravelledAbroad',
       'GovernmentSector']

target_column, feature_columns

# Kolom Travel Insurance
pd.value_counts(df_clean['TravelInsurance'])

# Memilih setiap labelnya
no  = df_clean[df_clean['TravelInsurance'] == 0]
yes = df_clean[df_clean['TravelInsurance'] == 1]

# Melakukan resample sehingga banyak data pada label = 1
df_resampled = resample(yes, replace = True, n_samples = 1277) 

# Memasukan datanya pada dataframe
df_clean = pd.concat([no, df_resampled])

pd.value_counts(df_clean['TravelInsurance'])

"""## 4) Pembagian Dataset"""

X_train, X_test, y_train, y_test = train_test_split(
    df_clean[feature_columns],
    df_clean[target_column],
    test_size=0.2,
    random_state=180,
    shuffle=True,
    stratify=df_clean[target_column],
    )

print(y_train.groupby(y_train).count())
print(y_test.groupby(y_test).count())

"""## 5) Standarisasi Data"""

# Inisialisasi fungsi MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

"""# Modeling
Seperti yang telah dituliskan dalam solution statement, model machine learning yang digunakan untuk menyelesaikan permasalahan dalam proyek ini adalah KNN dan Gradient Boosting.
1. KNN.

  Model KNN yang digunakan dari library sklearn ini memilih nilai hyperparameter k=8 yang berasal dari hasil pengujian beberapa nilai lainnya dari 1-10 dan memiliki performa paling baik. Model dilatih dengan data yang telah melewati pra-pemrosesan. 
2. Gradient Boosting.

  Model Gradient Boosting ini juga menggunakan library sklearn GradientBoostingClassifier dan dilatih dengan data yang telah melewati pra-pemrosesan.

## 6) Membuat Model KNN
"""

# Model KNN
knn = KNeighborsClassifier()

# Latih model
knn.fit(X_train, y_train)

"""## 7) Membuat Model Gradient Boosting """

# Model GradientBoosting
gradient_boosting = GradientBoostingClassifier()

# Latih model
gradient_boosting.fit(X_train, y_train)

# Simpan hasil akurasi baseline model
score = pd.DataFrame(columns=['latih', 'uji'], index=['KNN', 'Boosting'])
model_dict = {'KNN': knn, 'Boosting': gradient_boosting}
for name, model in model_dict.items():
    score.loc[name, 'latih'] = model.score(X_train, y_train)
    score.loc[name, 'uji'] = model.score(X_test, y_test)

score

"""## 8) Pengembangan model"""

# KNN
# Hyperparameter grid
param_grid = {'n_neighbors': [3, 5, 7, 9],
              'p': [1, 2],
              'weights': ["uniform","distance"],
              'algorithm':["ball_tree", "kd_tree", "brute"],
              }

# Pencarian parameter terbaik dengan GridSearchCV
knn_improved = GridSearchCV(knn, 
                            param_grid, 
                            cv = 5, 
                            verbose = 1, 
                            n_jobs = -1,
                            scoring='accuracy',
                            )
knn_improved.fit(X_train, y_train)

print(f"Parameter terbaik untuk KNN = {knn_improved.best_estimator_}")

# Pilih parameter terbaik untuk KNN
knn_improved = KNeighborsClassifier(**knn_improved.best_params_)

# Latih model final
knn_improved.fit(X_train, y_train)

# Gradient Boosting
# Hyperparameter grid
param_grid = {'n_estimators': [10, 50, 100, 200, 500, 750, 1000],
              'max_depth': [3, 5, 10],
              'min_samples_leaf': [np.random.randint(1,10)],
              'max_features': [None, 'sqrt', 'log2']
              }

# Pencarian parameter terbaik dengan GridSearchCV
gb_improved = GridSearchCV(gradient_boosting,
                        param_grid,
                        cv = 5, 
                        verbose = 1, 
                        n_jobs = -1,
                        scoring = 'roc_auc'
                        )
gb_improved.fit(X_train, y_train)

print(f"Parameter terbaik untuk Gradient Boosting = {gb_improved.best_estimator_}")

# Pilih parameter terbaik untuk Gradient Boosting
gb_improved = GradientBoostingClassifier(**gb_improved.best_params_)

# Latih model final
gb_improved.fit(X_train, y_train)

# Simpan hasil akurasi
score = pd.DataFrame(columns=['latih', 'uji'], index=['KNN', 'Boosting'])
model_dict = {'KNN': knn,
              'Boosting': gradient_boosting,
              'Improved KNN': knn_improved,
              'Improved Boosting': gb_improved}
for name, model in model_dict.items():
    score.loc[name, 'latih'] = model.score(X_train, y_train)
    score.loc[name, 'uji'] = model.score(X_test, y_test)

score

"""Dari hasil seluruh model yang dibuat, model Gradient Boosting yang dikembangkan memiliki nilai terbaik, oleh karena itu model ini akan digunakan pada tahap selanjutnya.

# Evaluation
Sebagai evaluasi, proyek klasifikasi akan menggunakan metrik *accuration*, *precision*, *recall*, dan *F1 score*. Kita juga akan melihat hasil *confusion matrix* dari prediksi model sebelum membahas empat metrik sebelumnya untuk lebih memberikan gambaran hasil evaluasi.
- *Confusion matrix* adalah matriks yang berisi 4 notasi tp, tn, fp, fn. Notasi tp (true positive) dan tn (true negative) menunjukkan jumlah nilai positif dan negatif yang diprediksi secara tepat. Sedangkan notasi fp (false positive) dan fn (false negative) menunjukkan jumlah nilai positif dan negatif yang diprediksi salah. Kelebihan matriks ini adalah paling sederhana untuk dipahami dan kekurangannya adalah tidak cukup informatif untuk mengukur hasil sehingga perlu diolah kembali [[3]](https://d1wqtxts1xzle7.cloudfront.net/37219940/5215ijdkp01-with-cover-page-v2.pdf?Expires=1633937501&Signature=Lh-E3CkynhdzyHhFcDlM1pOk9qvqGwALQZj0kzw6yIkgAJGQ0zMRVGdndKem94902lTQsbRfL8NNnjn594cOIKHaGrPScCkXO25enZRyRZZ8CeZEDDoQdxBrpUq1OFJxBvGFGouzKMsp5Wk~GfGSt4VuVAJIq2OmZhid06seH4ftWP7vGFpTp-XBvMD~r7qJ45MeI4gwO6nwkw0vnYnSxpY2VTCT7h6eIlrvXW9OM4JofLrIK2GXhyHABQDwlR4Ki2LO~uBOcuwLuQw9~3F6pj663yvoItaB8w~ObGcH~-C2G9Y288aEL~Xelbq8wL3b~S0eW1PxoSr9f1AlOxdXpQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA).

- *Accuration* atau akurasi adalah salah satu metriks evaluasi kesalahan yang sering dipakai. Akurasi didapatkan dari persentase prediksi yang benar terhadap total nilai yang ada. Kelebihan metrik ini adalah penilaian yang mudah digunakan, lebih sedikit kerumitan, bisa digunakan dalam multi label atau multi kelas, dan mudah dipahami. Sedangkan kekurangannya adalah keterbatasan dalam proses evaluasi dan proses diskriminasi. Contohnya adalah jika dalam sebuah dataset dengan nilai negatif yang berjumlah 80% dan model yang dibuat memprediksi seluruhnya negatif. Maka hasil akurasinya akan tetap bernilai 80% tanpa mengetahui bahwa ada bias kesalahan prediksi, yakni model selalu memprediksi negatif.

- *Precision* atau presisi adalah metriks evaluasi untuk mengukur pola positif yang diprediksi dengan benar dari total pola prediksi dalam kelas positif. Kelebihan presisi adalah mampu menilai prediksi model terhadap label data positif. Ini menghasilkan presisi tidak mampu mengukur hasil label negatif.

- *Recall* adalah metriks evaluasi untuk mengukur pola positif yang diprediksi dengan benar dari total pola prediksi yang benar. Metriks ini adalah nilai yang berlawanan dengan presisi sehingga memiliki keunggulan menghitung bagian negatif dari prediksi label positif dan kekurangannya adalah tidak mampu menghitung prediksi negatif.

- *f1-score* adalah metriks evaluasi yang menggunakan nilai presisi dan *recall* untuk mengukur seberapa baik hasil dan seberapa lengkap hasil prediksinya. Kombinasi presisi dan *recall* menjadikan *f1-score* saling melengkapi kekurangan dua evaluasi metriks tersebut namun tidak dapat menghitung hasil prediksi benar pada label negatif.

## 9) Confusion matriks
"""

y_pred = gb_improved.predict(X_test)

# Confusion matrix
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

print(f'True Positive: {tp}\nTrue Negative: {tn}\nFalse Positive: {fp}\nFalse Negative: {fn}')

"""## 10) Akurasi, presisi, recall, dan F1-score"""

print('Akurasi: ', round(accuracy_score(y_pred, y_test), 3) * 100, '%')
print('Presisi: ', round(precision_score(y_pred, y_test), 3)* 100, '%')
print('Recall: ', round(recall_score(y_pred, y_test), 2)* 100, '%')
print('F1-score: ', round(f1_score(y_pred, y_test), 3)* 100, '%')

